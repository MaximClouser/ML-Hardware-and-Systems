{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-HW-SYS/a2-MaximClouser/blob/main/2_size_estimator_and_profiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1A-uzhwpt_"
      },
      "source": [
        "# **2. Model Size Estimation**\n",
        "\n",
        "It is no surprise that with such a tiny package, your Ardunio Nano 33 BLE Sense comes with limited memory and processing power. Therefore, you must be aware of the size and components of your model in order to have it run efficiently on your MCU.\n",
        "\n",
        "This notebook explores how various neural network layers affect the number of parameters, the amount memory, the number of floating point operations, and the CPU runtime of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5kS114ooq-0"
      },
      "source": [
        "## 2.0 Setup GDrive and Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BgbZjaQZ8niT",
        "outputId": "2e8648b0-0973-41b6-fded-8e5b7f298ad4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sH-xe50YtFNd"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username\n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rRj2U4Y3ttgu",
        "outputId": "4696456c-0ed5-4eac-ef8f-122205ef2bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545\n",
            "fatal: destination path 'a2-MaximClouser' already exists and is not an empty directory.\n",
            "/content/gdrive/MyDrive/ece5545/a2-MaximClouser\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 79.69 KiB | 1.01 MiB/s, done.\n",
            "From https://github.com/ML-HW-SYS/a2-MaximClouser\n",
            "   2a1a2d2..41bc548  main       -> origin/main\n",
            "Updating 2a1a2d2..41bc548\n",
            "Fast-forward\n",
            " 3_training_and_analysis.ipynb | 490 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m------------\u001b[m\n",
            " 1 file changed, 401 insertions(+), 89 deletions(-)\n",
            "/content/gdrive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# %mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R9V2uP4YtzuR"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIzgKXcDtFNe"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ePTgb55gwT7o",
        "outputId": "9aa1fef0-0f9d-49c0-81b4-42c65460a45a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a2-MaximClouser\n",
            "constants.py  loaders.py   __pycache__\t\tsize_estimate.py\n",
            "data_proc.py  models\t   quant_conversion.py\ttrain_val_test_utils.py\n",
            "dataset       networks.py  quant.py\n",
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(PROJECT_ROOT)\n",
        "!ls {PROJECT_ROOT}/src\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2uKEHl-OtFNf",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "11daea15-c19e-40dc-cc7d-3d5fbad9e635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model folders are created, \n",
            "PyTorch models will be saved in /content/gdrive/MyDrive/ece5545/models/torch_models, \n",
            "ONNX models will be saved in /content/gdrive/MyDrive/ece5545/models/onnx_models, \n",
            "TensorFlow Saved Models will be saved in /content/gdrive/MyDrive/ece5545/models/tf_models, \n",
            "TensorFlow Lite models will be saved in /content/gdrive/MyDrive/ece5545/models/tflite_models, \n",
            "TensorFlow Lite Micro models will be saved in /content/gdrive/MyDrive/ece5545/models/micro_models.\n",
            "Imported code dependencies\n"
          ]
        }
      ],
      "source": [
        "import sys,os\n",
        "\n",
        "# Adding assignment 2 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nnt\n",
        "import src.data_proc as data_proc\n",
        "from src.constants import *\n",
        "import numpy as np\n",
        "\n",
        "print(\"Imported code dependencies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTKG-QIfwcN9"
      },
      "source": [
        "## 2.2 Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RRSPvUTyIQ2"
      },
      "source": [
        "### Create the model\n",
        "Our TinyConv model currently consists of 7 layers:\n",
        "\n",
        "\n",
        "1. [Reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html)\n",
        "2. [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
        "3. [Relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
        "4. [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n",
        "5. Reshape\n",
        "6. [Fully Connected (Linear)](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "7. [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)\n",
        "\n",
        "\n",
        "Please refer to `<github_dir>/src/networks.py` for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jU2ryBhlwcN_",
        "outputId": "2950201c-4a6a-4a5f-881f-1fe1f006fe24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda to run the training scrpit.\n"
          ]
        }
      ],
      "source": [
        "# Define device\n",
        "from src.networks import TinyConv\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} to run the training scrpit.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLwsdGt_liKa"
      },
      "source": [
        "### Create data_proc.AudioProcessor() object for data preprocessing\n",
        "When an AudioProcessor instance is created:\n",
        "\n",
        "1. Download speech_command dataset from DATA_URL (defined in constants.py) to data_dir (default: '/content/gdrive/MyDrive/ece5545/data')\n",
        "default dataset url: 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "\n",
        "2. Determine classes and their numerical indices for training and testing based on WANTED_WORDS\n",
        "(defined in constants.py):\n",
        "eg. if WANTED_WORDS is ['yes', 'no'], model will be trained to identify \"yes\" and \"no\" as yes and no,\n",
        "other words as unkown, and background noises as silence\n",
        "\n",
        "3. Determine and save the settings for data processing feature generator based on relavent constants\n",
        "in constants.py\n",
        "\n",
        "4. Determine which audio files in the dataset are for testing, training, or validating using hash method\n",
        "\n",
        "5. Prepare and save background noise data using the background noise data inside dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HLxcu6BB_wxg"
      },
      "outputs": [],
      "source": [
        "# Create audio processor (this takes some time the first time)\n",
        "# And continues to run for a bit after reaching 100% while it's extracting files\n",
        "audio_processor = data_proc.AudioProcessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_RJ8a0otJEJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e28474-7c90-4d73-eb94-6bd61ed7b000"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyConv(\n",
              "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
              "  (conv): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
              "  (fc): Linear(in_features=4000, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Create model\n",
        "model_fp32 = TinyConv(audio_processor.model_settings)\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcB7Ri8ewcOA"
      },
      "source": [
        "## 2.3 Model Estimates\n",
        "Run the next few cells to see how each layer impacts memory and runtime of the below TinyConv neural network model. Then experiment with reshaping it to see how adding or removing layers alters the metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEnwhmThwcOC"
      },
      "source": [
        "### Memory Utilization\n",
        "\n",
        "There are two important forms of memory that we care about for MCUs: **flash memory** and **random access memory (RAM)**. Flash is **non-volatile** aka persistent storage memory; its data is saved when powered off. This is where your model's weights and code live, thus they must be able to fit within the capacity of your MCU's flash memory (1MB). On the other hand, RAM is **volatile** or non-persistent memory, thus it is used for temporary storage like input buffers and intermediate tensors. Together, they cannot exceed the size of your RAM storage (256KB).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Gv8LHUtFNi"
      },
      "source": [
        "### TODO 1: Implement the `count_trainable_parameters` function in `src/size_estimate.py` to compute model size and get an estimate of the flash usage of this model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "--J01LrjtFNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48abf971-fd3f-47da-85ca-3ae7387dcd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total:  16652\n",
            "Total number of trainable parameters:  0.016652 M\n"
          ]
        }
      ],
      "source": [
        "# Sends model weights to the GPU if tensors are on GPU\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "from src.size_estimate import count_trainable_parameters\n",
        "num_params = count_trainable_parameters(model_fp32)\n",
        "print(\"Total: \", num_params)\n",
        "print(\"Total number of trainable parameters: \", num_params / float(1e6), \"M\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get estimated flash usage of model\n",
        "from src.size_estimate import calculate_model_size\n",
        "flash_size = calculate_model_size(model_fp32)\n",
        "print(flash_size)\n",
        "print(\"Estimated flash size: \", flash_size / float(1e6), \"MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijenYe7x2KNT",
        "outputId": "ae9bb6cb-d323-44d0-e36f-0edee3af5f02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66608\n",
            "Estimated flash size:  0.066608 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZaeZSLrtFNi"
      },
      "source": [
        "### TODO 2: Implement the `compute_forward_size` function in `src/size_estimate.py` to compute the memory needed for a forward pass. This is how much RAM you will be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tLJ-LBfo56IL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ece51a8-37ad-4ec3-94f2-4d88d5d114a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward memory:  116.765625 KB\n",
            "Forward memory:  0.1140289306640625 MB\n"
          ]
        }
      ],
      "source": [
        "# Sends model weights to the GPU if tensors are on GPU\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "from src.size_estimate import compute_forward_memory\n",
        "frd_memory = compute_forward_memory(\n",
        "    model_fp32,\n",
        "    (1, model_fp32.model_settings['fingerprint_width'], model_fp32.model_settings['spectrogram_length']),\n",
        "    device\n",
        ")\n",
        "print(\"Forward memory: \", frd_memory / 1024, \"KB\")\n",
        "print(\"Forward memory: \", frd_memory / (1024 ** 2), \"MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkjm2OlXgArr"
      },
      "source": [
        "As you can see above, the number of parameters in a neural network can add up fast which is a concern when dealing with a small amount of RAM. With the TinyConv neural network only consuming 0.21MB out of 1MB, our model will easily fit within flash memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4qkx4L8hCBD"
      },
      "source": [
        "### Number of Operations\n",
        "\n",
        "### TODO 3: Implement the `flop` function in `src/size_estimate.py` to count the total FLOPS in a forward pass with batch size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Mb5ugZA3wcOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c945e972-65bd-4707-8ad3-7ca966437af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'conv_reshape': {}, 'conv': {'conv': 640000}, 'relu': {}, 'dropout': {}, 'fc_reshape': {}, 'fc': {'fc': 32004}, '': {}}\n",
            "total number of floating operations: 672004\n",
            "Number of FLOPs by layer and parameters:\n",
            "Conv:  {'conv': 640000}\n",
            "FC:    {'fc': 32004}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from src.size_estimate import flop\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "# The total number of floating point operations\n",
        "flop_by_layers = flop(\n",
        "    model=model_fp32,\n",
        "    input_shape=(\n",
        "        1,\n",
        "        model_fp32.model_settings['fingerprint_width'],\n",
        "        model_fp32.model_settings['spectrogram_length']\n",
        "    ),\n",
        "    device=device)\n",
        "total_param_flops = sum([sum(val.values()) for val in flop_by_layers.values()])\n",
        "\n",
        "print(flop_by_layers)\n",
        "print(f'total number of floating operations: {total_param_flops}')\n",
        "print('Number of FLOPs by layer and parameters:')\n",
        "print(\"Conv: \", flop_by_layers['conv'])\n",
        "print(\"FC:   \", flop_by_layers['fc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbAbpr4Mj12i"
      },
      "source": [
        "### CPU runtime\n",
        "\n",
        "### TODO 4: Measure the server/desktop CPU runtime to compare to the MCU runtime later in this assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zMcAtzKHwcOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8de27e-fed8-4000-9610-93526d6ec69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "             model_inference        43.74%     489.000us       100.00%       1.118ms       1.118ms             1            --  \n",
            "                aten::conv2d         0.89%      10.000us        39.36%     440.000us     440.000us             1       640.000  \n",
            "           aten::convolution         6.44%      72.000us        38.46%     430.000us     430.000us             1            --  \n",
            "          aten::_convolution         1.88%      21.000us        32.02%     358.000us     358.000us             1            --  \n",
            "    aten::mkldnn_convolution        27.28%     305.000us        30.14%     337.000us     337.000us             1            --  \n",
            "                aten::linear         1.25%      14.000us         8.86%      99.000us      99.000us             1            --  \n",
            "                 aten::addmm         4.29%      48.000us         5.72%      64.000us      64.000us             1        32.000  \n",
            "               aten::reshape         1.07%      12.000us         3.31%      37.000us      18.500us             2            --  \n",
            "                  aten::relu         1.70%      19.000us         3.13%      35.000us      35.000us             1            --  \n",
            "                  aten::view         2.24%      25.000us         2.24%      25.000us      12.500us             2            --  \n",
            "                     aten::t         0.98%      11.000us         1.88%      21.000us      21.000us             1            --  \n",
            "               aten::softmax         0.36%       4.000us         1.61%      18.000us      18.000us             1            --  \n",
            "             aten::clamp_min         1.43%      16.000us         1.43%      16.000us      16.000us             1            --  \n",
            "              aten::_softmax         1.25%      14.000us         1.25%      14.000us      14.000us             1            --  \n",
            "               aten::resize_         1.16%      13.000us         1.16%      13.000us      13.000us             1            --  \n",
            "                 aten::empty         0.98%      11.000us         0.98%      11.000us       5.500us             2            --  \n",
            "             aten::transpose         0.63%       7.000us         0.89%      10.000us      10.000us             1            --  \n",
            "                 aten::copy_         0.89%      10.000us         0.89%      10.000us      10.000us             1            --  \n",
            "           aten::as_strided_         0.72%       8.000us         0.72%       8.000us       8.000us             1            --  \n",
            "                aten::expand         0.45%       5.000us         0.54%       6.000us       6.000us             1            --  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.118ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_fp32.cpu()\n",
        "model_fp32.eval()\n",
        "inputs = torch.rand([1,1960]).cpu()\n",
        "\n",
        "# Run a profiler to see the cpu time for inference\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops=True, with_stack=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model_fp32(inputs)\n",
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_time(model, inputs, device):\n",
        "    model.to(device)\n",
        "    inputs = inputs.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    activities = [ProfilerActivity.CPU]\n",
        "    if device.type == 'cuda':\n",
        "        activities.append(ProfilerActivity.CUDA)\n",
        "\n",
        "    with profile(activities=activities, profile_memory=True, record_shapes=True) as prof:\n",
        "        with record_function(\"model_inference\"):\n",
        "            model(inputs)\n",
        "    print(prof.key_averages().table(sort_by=\"self_cpu_time_total\" if device.type == 'cpu' else \"self_cuda_time_total\", row_limit=10))\n",
        "\n",
        "inputs = torch.rand([1, 1960])\n",
        "measure_inference_time(model_fp32, inputs, torch.device('cpu'))\n",
        "if torch.cuda.is_available():\n",
        "    measure_inference_time(model_fp32, inputs, torch.device('cuda'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeUvih-GMy4U",
        "outputId": "c2d436d5-a062-41cf-ce9e-0c7facc346a1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "             model_inference        48.55%     584.000us       100.00%       1.203ms       1.203ms           0 b     -31.28 Kb             1  \n",
            "    aten::mkldnn_convolution        24.27%     292.000us        26.27%     316.000us     316.000us      15.62 Kb           0 b             1  \n",
            "                 aten::addmm         5.49%      66.000us         6.73%      81.000us      81.000us          16 b          16 b             1  \n",
            "           aten::convolution         4.41%      53.000us        32.17%     387.000us     387.000us      15.62 Kb           0 b             1  \n",
            "                  aten::view         2.33%      28.000us         2.33%      28.000us      14.000us           0 b           0 b             2  \n",
            "             aten::clamp_min         2.00%      24.000us         2.00%      24.000us      24.000us      15.62 Kb      15.62 Kb             1  \n",
            "              aten::_softmax         1.75%      21.000us         1.75%      21.000us      21.000us          16 b          16 b             1  \n",
            "                  aten::relu         1.58%      19.000us         3.57%      43.000us      43.000us      15.62 Kb           0 b             1  \n",
            "          aten::_convolution         1.50%      18.000us        27.76%     334.000us     334.000us      15.62 Kb           0 b             1  \n",
            "                 aten::empty         1.16%      14.000us         1.16%      14.000us       7.000us      15.62 Kb      15.62 Kb             2  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.203ms\n",
            "\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                aten::cudnn_convolution        84.64%       3.036ms        85.53%       3.068ms       3.068ms      17.000us        48.57%      17.000us      17.000us           0 b           0 b      16.00 Kb      16.00 Kb             1  \n",
            "void implicit_convolve_sgemm<float, float, 128, 5, 5...         0.00%       0.000us         0.00%       0.000us       0.000us      17.000us        48.57%      17.000us      17.000us           0 b           0 b           0 b           0 b             1  \n",
            "                                            aten::addmm         1.87%      67.000us         2.37%      85.000us      85.000us       8.000us        22.86%       8.000us       8.000us           0 b           0 b         512 b         512 b             1  \n",
            "                                             aten::add_         0.67%      24.000us         1.00%      36.000us      36.000us       4.000us        11.43%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
            "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us        11.43%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
            "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us        11.43%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
            "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us        11.43%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
            "                                        aten::clamp_min         0.59%      21.000us         0.84%      30.000us      30.000us       3.000us         8.57%       3.000us       3.000us           0 b           0 b      16.00 Kb      16.00 Kb             1  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         8.57%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
            "                                         aten::_softmax         0.53%      19.000us         0.75%      27.000us      27.000us       3.000us         8.57%       3.000us       3.000us           0 b           0 b         512 b         512 b             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.587ms\n",
            "Self CUDA time total: 35.000us\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/gdrive/MyDrive/ece5545/a2-MaximClouser/src\n",
        "# !git config --global user.email \"maximclouser@gmail.com\"\n",
        "# !git config --global user.name \"MaximClouser\"\n",
        "# !git commit -am \"fixed size func\"\n",
        "# # !git push\n",
        "# !git push --force origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZtrZKhfh1Yt",
        "outputId": "8374e5f4-961f-4a89-c8ed-ff4a7d1a6df3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a2-MaximClouser/src\n",
            "[main 6e7caec] fixed size func\n",
            " 1 file changed, 2 insertions(+), 2 deletions(-)\n",
            "Enumerating objects: 7, done.\n",
            "Counting objects: 100% (7/7), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 408 bytes | 81.00 KiB/s, done.\n",
            "Total 4 (delta 3), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
            "To https://github.com/ML-HW-SYS/a2-MaximClouser.git\n",
            "   41bc548..6e7caec  main -> main\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2_size_estimator_and_profiler.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "92bf126df007708fd70c442c808ee74575bedf7ea6317e0b182c3af0184af25d"
    },
    "kernelspec": {
      "display_name": "ece5545",
      "language": "python",
      "name": "ece5545"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}